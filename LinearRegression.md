<div align='center' ><font size='70'>Linear Regression</font></div>

# 简介

**线性回归模型**是用一条曲线去拟合一个或多个自变量$x$与因变量$y$之间关系的模型，若曲线是一条直线或超平面时是线性回归（直线时是一元线性回归；超平面时是多元线性回归），否则是非线性回归，常见的非线性回归有多项式回归和逻辑回归。

线性回归通过学习样本
$$
D={\{(x_i, y_i)}\}_{i=1}^m
$$
得到一种映射关系：
$$
f:x\rightarrow y
$$
得到的预测结果$\hat y$是连续值。线性回归是机器学习中基础算法，同时也是高级算法的根基，所以有必要掌握它的思想。接下来将详细叙述该算法。

# 基本形式

**预测函数一般表示式**
$$
f(x_i)=\theta_0+\theta_1x_i^{(1)}+\theta_2x_i^{(2)}+...+\theta_mx_i^{(m)}
$$
令$x_i^{(0)}=1$上式可以写成：
$$
f(x_i)=\theta_0x_i^{(0)}+\theta_1x_i^{(1)}+\theta_2x_i^{(2)}+...+\theta_mx_i^{(m)}=\sum_{j=0}^{m}\theta_jx_i^{(j)}
$$
其中$x_i^{(j)}, j\in(1,2,...,m)$表示第$i$个样本中的第$j$个特征。

**向量表示式**
$$
f(X)=\Theta^TX
$$
其中$\Theta^T=[\theta_0,\theta_1, \theta_2,...,\theta_m]$，$X=[x_1, x_2, x_3,...,x_n]^T$，$x_i=[x_i^{(0)}, x_i^{(1)}, x_i^{(2)},...,x_i^{(m)}]$。需要明确的是，向量表示式的展开形式一定和一般式一致，写成向量表示需要注意行和列的关系。

# 损失函数

损失函数用来表征当前模型的拟合效果，我们自然是希望通过$f(x)$预测得到的$\hat y$能够和真实值$y$相差不大（过拟合的问题在后面会考虑）。

一般将线性回归的损失函数定义为：
$$
L(\theta_0,\theta_1,...,\theta_m)=\frac{1}{2n}\sum_{i=1}^{n}(f_\theta(x_i)-y_i)^2
$$
这里的$\frac{1}{2}$是为了求导计算的便利，而$\frac{1}{n}$是将损失平均化，消除样本量$n$带来的影响。$f_\theta(x_i)$是预测值，$y_i$代表第$i$个样本对应的真实值，那么预测值和真实值之间的差距，我们用$\epsilon$来表示，即$\epsilon=f_\theta(x_i)-y_i$。

注意，不要忘记我们的目的，我们希望预测值能够接近真实值，那么从损失函数的角度上看，就是我们希望找到一组合适的参数$(\theta_0,\theta_1,\theta_2,...,\theta_m)$使得我们的损失函数最小。

那么，我们为什么要使用均方误差（MSE）来作为损失函数呢？使用均方根误差（RMSE）行不行？使用平均绝对误差（MAE）行不行？

答案当然是可以的，任何一种方式都是为了表征预测值与真实值之间的差距，通过梯度下降的优化方法是可以通过不断地迭代找到最优解的。而且，对于线性回归问题而言，最优解有且仅有一组，所以不论采用哪种计算方式作为损失函数，对于最终的最优解是没有影响的。

使用均方误差（MSE）可以从概率的角度进行诠释（极大似然估计，假设误差$\epsilon$是独立同分布且服从高斯分布），这部分在[学习资料](https://github.com/datawhalechina/team-learning/blob/master/机器学习算法基础/Task1%Linear_regression.ipynb)上有，此处不再赘述。

# 正则化

其实，对于线性回归模型而言，本身不需要考虑正则化问题，因为线性回归模型的参数$\theta$的数量始终应该是输入样本$x_i$的特征数量再加1（请认真思考这句话）。

当开始为样本构造新的特征时，如加入$x_i^2,x_i^3$等特征多项式时，此时的模型被称为**广义线性模型**（而非线性回归模型），模型的参数$\theta$的数量也会增加。通过[泰勒公式](https://baike.baidu.com/item/泰勒公式)我们可以知道，一个复杂的多项式可以拟合任何函数。对于有限的样本集而言（我们得到的样本数据总是有限的），通过不断地增加高阶特征多项式，从而使得我们的模型非常好得拟合每一个样本点。但这是不是我们想要的呢？

显然不是，首先，我们得到的样本数据本身是有限的，它只能表现总体样本的一部分信息；其次，样本数据本身带有噪声（受到采样精度、环境因素等等的影响）。我们更希望从有限的样本数据中"窥探"出总体样本的趋势，所以我们一方面希望模型能够较好的拟合样本，另一方面也希望模型不要过度拟合样本（即发生过拟合）。

根据[奥卡姆剃刀原理](https://baike.baidu.com/item/奥卡姆剃刀原理/10900565?fr=aladdin)，我们同样认为一个好的模型在使用较少参数时依然能够有着令人满意是表现，从而引入正则化项来对模型参数数量产生约束。（注意：正则化是一种思想，并不是只有L1正则、L2正则。凡是对模型复杂度进行约束的手段，都可以认为是引入正则化）

# 目标函数

将损失函数和正则化结合起来，就是最终的目标函数。
$$
J=argmin\ L(\Theta)+\lambda H(F)
$$
其中，$H(F)$表示模型的复杂度，与$\Theta$相关。

# 优化方法

明确了目标函数，接下来就是优化方法。对于线性回归模型而言，能够利用矩阵直接求解最优值。但对于大多数复杂的模型（如深度学习模型），面对海量数据时（直接求最优解意味着需要一次性输入所有训练数据，需要大量内存，且大型矩阵运算时十分消耗计算资源），往往采用梯度下降的方法不断更新参数，最终取得最优解。

具体的梯度下降优化算法有很多，如常见的随机梯度下降（SGD）、神经网络中常用的Adam等，这些都可参阅本人的[知乎文章](https://zhuanlan.zhihu.com/p/110104333)，对几乎常用的优化算法都进行了梳理。

