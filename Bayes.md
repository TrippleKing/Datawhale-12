<center>Bayes</center>

# 基本概念

监督学习的任务就是学习一个模型，应用这一模型，对给定的输入预测相应的输出。这个模型的一般形式为决策函数：
$$
Y=f(X)
$$

或者条件概率分布：
$$
P(Y|X)
$$
监督学习方法又可以分为生成方法（Generative Approach）和判别方法（Discriminative Approach）。所学到的模型分别称为**生成模型**（Generative Model）和**判别模型**（Discriminative Model）。

- <font color=red>生成方法</font>**由数据学习联合概率分布**$P(X,Y)$，然后求出条件概率分布$P(Y|X)$作为预测的模型，即**生成模型**：

$$
P(Y|X)=\frac{P(X,Y)}{P(X)}
$$

- 这样的方法之所以称为生成方法，是因为**模型表示了给定输入$X$产生输出$Y$的生成关系**。典型的生成模型有朴素贝叶斯法和隐马尔科夫模型。

- <font color=red>判别方法</font>由**数据直接学习决策函数$f(X)$或条件概率分布**$P(Y|X)$作为预测的模型，即判别模型。判别方法关心的是对给定的输入$X$，应该预测什么样的输出$Y$。典型的判别模型包括：K近邻法、感知机、决策树、逻辑回归、最大熵模型、支持向量机、提升方法和条件随机场等。

在监督学习中，生成方法和判别方法各有优缺点，适合于不同条件下的学习问题。

- **生成方法的特点**：生成方法可以**还原出联合概率分布**$P(X,Y)$，而判别方法则不能；生成方法的**学习收敛速度更快**，即当样本容量增加的时候，学到的模型可以更快地收敛于真实模型；当存在[隐变量](https://www.zhihu.com/question/43216440)时，仍可以用生成方法学习，此时判别方法就不能用。

- **判别方法的特点**：判别方法直接学习的是条件概率$P(Y|X)$或决策函数$f(X)$，直接面对预测，往往**学习的准确率更高**；由于直接学习$P(Y|X)$或$f(X)$，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。

# 朴素贝叶斯法

朴素贝叶斯法是基于**贝叶斯定理**与**特征条件独立假设**的分类方法。（注意：朴素贝叶斯法与贝叶斯估计是不同的概念）对于给定的训练数据集，首先基于特征条件独立假设学习输入输出的联合概率分布；然后基于此模型，对给定的输入$x$，利用贝叶斯定理求出后验概率最大的输出$y$。朴素贝叶斯法实现简单，学习与预测的效率都很高，是一种常用的方法。

## 基本方法

设输入空间$X\in R^n$为$n$维向量的集合，输出空间为类标记集合$Y=\{c_1,c_2,...,c_K\}$。输入为特征向量$x\in X$，输出为类标记（class label）$y\in Y$。训练数据集
$$
T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}
$$
由$P(X,Y)$独立同分布产生。

朴素贝叶斯法通过训练数据集学习联合概率分布$P(X,Y)$。具体地，学习一下先验概率分布及条件概率分布。先验概率分布
$$
P(Y=c_k), \ k=1,2,...,K
$$
条件概率分布
$$
P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_k), \\ k=1,2,...,K
$$
于是学习到联合概率分布$P(X,Y)$。

条件概率分布$P(X=x|Y=c_k)$有指数级数量的参数，其估计实际是不可行的。事实上，假设$x^{(j)}$可取值有$S_j$个，$j=1,2,...,n$，$Y$可取值有$K$个，那么参数个数为$K\prod\limits_{j=1}^nS_j$。

朴素贝叶斯法对条件概率分布作了**条件独立性的假设**。由于这是一个较强的假设，朴素贝叶斯法也由此得名。具体地，条件独立性假设是
$$
\begin{align*}
P(X=x|Y=c_k)=&P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_k)\\
=&\prod\limits_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)
\end{align*}
$$
朴素贝叶斯实际上学习到生成数据的机制，所以属于生成模型。条件独立假设等于是说用于分类的特征在类确定的条件下都是条件独立的。这一假设使朴素贝叶斯法变得简单，但会牺牲一定的分类准确率。

朴素贝叶斯法分类时，对给定的输入$x$，通过学习到的模型计算后验概率分布$P(Y=c_x|X=x)$，将后验概率最大的类作为$x$的类输出。后验概率计算根据贝叶斯定理进行：
$$
\begin{align*}
P(Y=c_k|X=x)=&\frac{P(X=x|Y=c_k)P(Y=c_k)}{\sum\limits_kP(X=x|Y=c_k)P(Y=c_k)}\\
=&\frac{P(Y=c_k)\prod\limits_jP(X^{(j)}=x^{(j)}|Y=c_k)}{\sum\limits_kP(Y=c_k)\prod\limits_jP(X^{(j)}=x^{(j)}|Y=c_k)},\\
&k=1,2,...,K
\end{align*}
$$
这是朴素贝叶斯分类的基本公式。于是，朴素贝叶斯分类器可表示为
$$
y=f(x)=arg\max\limits_{c_k}\frac{P(Y=c_k)\prod\limits_jP(X^{(j)}=x^{(j)}|Y=c_k)}{\sum\limits_kP(Y=c_k)\prod\limits_jP(X^{(j)}=x^{(j)}|Y=c_k)}
$$
注意到，在上式中分母对所有$c_k$都是相同的，所以，
$$
y=f(x)=arg\max\limits_{c_k}{P(Y=c_k)\prod\limits_jP(X^{(j)}=x^{(j)}|Y=c_k)}
$$

# 后验概率最大化的含义

朴素贝叶斯法将实例分到后验概率最大的类中。这等价于期望风险最小化。假设选择$0-1$损失函数：
$$
\begin{equation}
L(Y,f(X))=
 \{
\begin{array}{rcl}
1, & & {Y\neq f(X)}\\
0, & & {Y=f(X)}\\
\end{array} 
\end{equation}
$$
式中$f(X)$是分类决策函数。这时，期望风险函数为
$$
R_{exp}(f)=E[L(Y,f(X))]
$$
期望是对联合分布$P(X,Y)$取的。由此条件期望
$$
R_{exp(f)}=E_X\sum\limits_{k=1}^K[L(c_k,f(X))]P(c_k|X)
$$
为了使期望风险最小化，只需对$X=x$逐个极小化，由此得到：
$$

\begin{align*}
f(x)=&arg \min \limits_{y\in Y}\sum\limits_{k=1}^KL(c_k,y)P(c_k|X=x)\\
=&arg \min \limits_{y\in Y}\sum\limits_{k=1}^KP(y\neq c_k|X=x)\\
=&arg\min\limits_{y\in Y}(1-P(y=c_x|X=x))\\
=&arg\max\limits_{y\in Y}P(y=c_k|X=x)
\end{align*}
$$


这样一来，根据期望风险最小化准则就得到了后验概率最大化准则：
$$
f(x)=arg\max\limits_{c_k}P(c_k|X=x)
$$
即朴素贝叶斯法所采用的原理。