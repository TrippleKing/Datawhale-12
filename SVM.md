Support Vector Machine（SVM）——Part A

By Xyao

# 前言

支持向量机是深度学习还未蓬勃发展之前，机器学习算法中最重要的支柱，学习支持向量机可以说是机器学习之路上重要而又困难的一个关卡。本文希望帮助读者能够清晰梳理支持向量机的"脉络"，学习起来可以清楚且轻松。

完整梳理支持向量机，内容也是相当多的，因此计划分为三部分进行叙述。

支持向量机有三宝：间隔(Margin)、对偶(Dual)、核技巧(Kernel Trick)，这"三宝"也是完整理解SVM的关键所在。

根据训练数据的特性，SVM可以分为三类：

1. **硬间隔支持向量机**（Hard Margin SVM）：面对的训练数据为**线性可分**。
2. **软间隔支持向量机**（Soft Margin SVM）：面对的训练数据为**近似线性可分**。
3. **核技巧支持向量机**（Kernel Trick SVM）：面对的训练数据为**线性不可分**。

我们从最基础的硬间隔支持向量机来开启SVM学习之路。

# 硬间隔支持向量机

硬间隔支持向量机面对的训练数据是线性可分的，我们先来看一个简单的例子：

<img src="https://i.loli.net/2020/05/01/RlCweKtkJGsDvuI.jpg" alt="SVM_1.JPG" style="zoom:50%;" />

如上图所示，在这样一个数据集中，存在多个可以将数据集正确划分为两类的超平面。那么。哪个超平面最好呢？或者说该如何去选择最优的超平面呢？我们直观上看，应该去找位于两类训练数据"正中间"的超平面（图中为加粗的黑线），因为这样的超平面对训练数据本身的局部扰动拥有最好的"容忍性"。

- 问：为什么这么说？或者如何理解"容忍性"？
- 答：由于训练数据的局限性或者噪声的影响，训练集外的数据可能比上图中的数据更接近两个类的分隔界面，这将使得许多超平面划分错误，而加粗的黑线所代表的超平面受影响最小，即具有最好的"容忍性"，或者说泛化能力最强。

这样的划分超平面可以通过如下线性方程来描述：
$$
w^Tx+b=0
$$
从而SVM模型可以定义为：
$$
f_w(x)=sign(w^Tx+b)
$$
其中，$sign()$函数称为分类决策函数，对于某样本$(x_i,y_i)$，若$w^Tx_i+b>0$，则$sign(w^Tx_i+b)=+1$；若$w^Tx_i+b<0$，则$sign(w^Tx_i+b)=-1$。

稍微总结一下，可以说，SVM的**核心问题**是如何找到"容忍性"最好的超平面。那么，怎样用数学语言来对<"容忍性"最好>这一概念进行描述呢？SVM就提出了**最大间隔分类器**的思想。

~~~gfm
```mermaid
graph LR
A[最大] -->B[Maximum]
C[间隔] --> D[间隔函数]
E[分类器] -->F[sign()函数]
```
~~~

